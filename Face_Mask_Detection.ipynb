{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df87a87",
   "metadata": {},
   "source": [
    "# Real Time Face Mask Detection using VGG16 CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f627922",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The project aims to detect whether a person is wearing a face mask or not using computer vision and machine learning techniques. The project utilizes the VGG16 pre-trained deep learning model and a custom dataset consisting of images with and without masks. The project is implemented using the Keras deep learning framework, OpenCV, and Python.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "915c2f4e",
   "metadata": {},
   "source": [
    "# Dataset Collection\n",
    "\n",
    "Data set consists of 7553 RGB images in 2 folders as with_mask and without_mask. Images are named as label with_mask and without_mask. Images of faces with mask are 3725 and images of faces without mask are 3828. This dataset is taken from following link:https://www.kaggle.com/datasets/omkargurav/face-mask-dataset ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fa125d",
   "metadata": {},
   "source": [
    "Let's start with importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b3c8799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741b7b8a",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "The data consists of two classes: with_mask and without_mask. For this experiment, The dataset is created by iterating through all the images in the respective directories using the OpenCV library. The images are resized to 224x224 pixels, as required by the VGG16 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cddda8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['with_mask', 'without_mask']\n",
    "#Creating a custom dataset\n",
    "data = []\n",
    "for category in categories:\n",
    "    path = os.path.join('/Users/Abdul/Desktop/project_fmd/data/train', category)\n",
    "    \n",
    "    label = categories.index(category)\n",
    "    \n",
    "    for file in os.listdir(path):\n",
    "        \n",
    "        img_path = os.path.join(path,file)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img,(224,224)) #VGG16 takes images of size 224x224\n",
    "        \n",
    "        data.append([img,label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff186a77",
   "metadata": {},
   "source": [
    "The dataset is then shuffled randomly to reduce any bias during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "030b3a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)\n",
    "# Creating a numpy array\n",
    "X=[]\n",
    "y=[]\n",
    "\n",
    "for features, label in data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "    \n",
    "X =np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6db5d4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7553, 224, 224, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Images, Width, Height, Number of Channels\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b89eddf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7553,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9c28b0",
   "metadata": {},
   "source": [
    "**X is the NumPy array that contains the images of the custom dataset.** \n",
    "\n",
    "In order to normalize the image pixel values to a common scale, each pixel value in the NumPy array X is divided by 255.\n",
    "\n",
    "Since the range of pixel values in an image is between 0 to 255, dividing each pixel value by 255 scales the pixel values to a range of 0 to 1. This is also known as feature scaling or normalization. Normalizing the pixel values of the images is a common preprocessing step in deep learning models, as it helps to reduce the impact of different scales of input features on the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56be9ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling on X\n",
    "X = X/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a38ad9b",
   "metadata": {},
   "source": [
    "**The train_test_split method from the scikit-learn library is used to split the custom dataset into training and testing sets.** \n",
    "\n",
    "The train_test_split method takes the feature data X and the label data y as input and returns four sets of data: X_train, X_test, y_train, and y_test.\n",
    "\n",
    "The first argument to the train_test_split method is the feature data X, and the second argument is the label data y. The test_size parameter is set to 0.2, which means that 20% of the data is used for testing, and the remaining 80% is used for training.\n",
    "\n",
    "The train_test_split method shuffles the data and splits it into two sets: the training set and the testing set. The X_train and y_train sets contain 80% of the data, and they are used to train the model. The X_test and y_test sets contain the remaining 20% of the data, and they are used to evaluate the performance of the model.\n",
    "\n",
    "Splitting the dataset into training and testing sets is an important step in building machine learning models, as it helps to evaluate the performance of the model on new, unseen data. By evaluating the model on the testing set, we can estimate how well the model will perform on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdef8c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing the train and test split on 2D Numpy Array\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b115795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6042, 224, 224, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6db3ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1511, 224, 224, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e6e0297",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "The VGG16 model is used as the base model for this project. The last layer of the VGG16 model is replaced with a Dense layer with a sigmoid activation function. The Dense layer consists of one neuron as the output is binary, i.e., whether the person is wearing a mask or not. The weights of the VGG16 model are frozen to avoid overfitting.\n",
    "\n",
    "Now, Let's import the pre-trained VGG16 convolutional neural network (CNN) architecture using Keras, which is a widely-used CNN architecture for image recognition tasks.\n",
    "\n",
    "Keras is an open-source deep learning library that provides a high-level API for building deep learning models. The keras.applications module provides a set of pre-trained deep learning models that can be used as a starting point for building custom models.\n",
    "\n",
    "The VGG16 model consists of 16 layers and has been trained on the ImageNet dataset, which contains millions of images from thousands of categories. The model is composed of a stack of convolutional layers followed by a set of fully connected layers. The convolutional layers are used to extract features from the input image, while the fully connected layers are used for classification.\n",
    "\n",
    "By importing the VGG16 model from Keras, we use this pre-trained model as a starting point for building a custom model for the face mask detection task. The VGG16 model is typically used as a feature extractor, where the output of the final convolutional layer is used as input to a set of fully connected layers that are trained for the specific task. In this code, the final layer of the VGG16 model is replaced with a single dense layer with a sigmoid activation function, which is trained for the face mask detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfc20961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c65c7244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 09:35:01.993304: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-04 09:35:01.995996: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "vgg = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f3c733e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "715d2bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15130aee",
   "metadata": {},
   "source": [
    "# The Sequential model \n",
    "\n",
    "It is a simple way to build neural networks that have a linear stack of layers. The Sequential model is appropriate for building feedforward neural networks, where the output of one layer is passed as input to the next layer in a sequential manner. It is not appropriate for building neural networks with multiple inputs or outputs, or with shared layers.\n",
    "\n",
    "In this code, a Sequential model is used to create a custom model for the face mask detection task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2f7ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Functional VGG into Sequential VGG\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdc095f",
   "metadata": {},
   "source": [
    "The layers of the VGG16 model are added to a Sequential model created for the face mask detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0482a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the last Layer of VGG according to our classification problem\n",
    "for layer in vgg.layers[:-1]:\n",
    "    model.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c0616bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f62e3b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing parameters\n",
    "for layer in model.layers:\n",
    "    layer.trainable =False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39e6db91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 0\n",
      "Non-trainable params: 134,260,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9f439b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ada570",
   "metadata": {},
   "source": [
    "The VGG16 model is used as a starting point, and the final layer of the VGG16 model is replaced with a single dense layer with a sigmoid activation function, which is trained for the face mask detection task. The layers of the VGG16 model are added to the Sequential model using the add method. The Sequential model is then compiled and trained using the compile and fit methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ad12470",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4123d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 134,264,641\n",
      "Trainable params: 4,097\n",
      "Non-trainable params: 134,260,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 4096 +1 bias = Trainable parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77b6624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bc6d9e9",
   "metadata": {},
   "source": [
    "# Training \n",
    "\n",
    "The model is trained using the binary cross-entropy loss function and the Adam optimizer. The model is trained for five epochs, and the validation data is used to evaluate the performance of the model. The train_test_split function from the scikit-learn library is used to split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "172be285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6042 samples, validate on 1511 samples\n",
      "Epoch 1/5\n",
      "6042/6042 [==============================] - 3510s 581ms/step - loss: 0.4381 - accuracy: 0.8128 - val_loss: 0.3335 - val_accuracy: 0.8511\n",
      "Epoch 2/5\n",
      "6042/6042 [==============================] - 3837s 635ms/step - loss: 0.2692 - accuracy: 0.9048 - val_loss: 0.2272 - val_accuracy: 0.9272\n",
      "Epoch 3/5\n",
      "6042/6042 [==============================] - 3840s 636ms/step - loss: 0.2181 - accuracy: 0.9220 - val_loss: 0.2011 - val_accuracy: 0.9285\n",
      "Epoch 4/5\n",
      "6042/6042 [==============================] - 3761s 622ms/step - loss: 0.1919 - accuracy: 0.9333 - val_loss: 0.1937 - val_accuracy: 0.9279\n",
      "Epoch 5/5\n",
      "6042/6042 [==============================] - 3670s 607ms/step - loss: 0.1812 - accuracy: 0.9351 - val_loss: 0.1618 - val_accuracy: 0.9431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc401e61e50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train, epochs=5, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ec778e",
   "metadata": {},
   "source": [
    "# Saving the Model\n",
    "\n",
    "The trained model is saved in the Hierarchical Data Format (HDF5) file format with the name saved_model.h5.\n",
    "\n",
    "Saving the model is an essential step because we can reuse the trained model in the future without retraining it. Additionally, we can deploy the trained model on different devices or platforms.\n",
    "\n",
    "The saved model contains the architecture of the model, its weights, and optimizer state, making it easy to reload the model for further training or inference. We can load the saved model using the load_model() method of the Keras library. The saved model file can also be converted to other formats, such as TensorFlow Lite, for use in mobile or edge devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b308cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/Users/surajbhardwaj/Desktop/project_fmd/data/saved_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd58e98a",
   "metadata": {},
   "source": [
    "# Real-time Face Mask Detection\n",
    "\n",
    "Now let's implement a real-time face mask detector using our trained model and OpenCV.\n",
    "\n",
    "The function detect_face_mask() takes an image as input, preprocesses it by resizing it to 224x224x3, predicts whether the face in the image is wearing a mask or not using the trained model, and returns the prediction.\n",
    "\n",
    "The function draw_label() takes the input image, text to be drawn, position of the text, and background color as arguments. It draws a rectangle with the specified background color and text on the image at the specified position using OpenCV.\n",
    "\n",
    "## Face Detection\n",
    "The haar object creates a Haar Cascade classifier object for face detection. The detect_face() function takes an image as input, converts it to grayscale, detects faces in the image using the Haar Cascade classifier, and returns the coordinates of the faces in the image.\n",
    "\n",
    "The code captures video using the cv2.VideoCapture() method and iterates over each frame of the video. For each frame, the code resizes the image, detects faces using the detect_face() method, and predicts whether each face is wearing a mask or not using the detect_face_mask() method. If a face is detected, a rectangle is drawn around the face using the cv2.rectangle() method. A label is also drawn on the image using the draw_label() method to indicate whether the face is wearing a mask or not. Finally, the image is displayed in a window using the cv2.imshow() method. The code exits if the user presses the 'x' key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37c54ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face_mask(img):\n",
    "    \n",
    "    y_pred = model.predict_classes(img.reshape(1,224,224,3))\n",
    "    \n",
    "    return y_pred[0][0]\n",
    "\n",
    "def draw_label(img,text,pos,bg_color):\n",
    "    \n",
    "    text_size = cv2.getTextSize(text,cv2.FONT_HERSHEY_SIMPLEX,1,cv2.FILLED)\n",
    "    \n",
    "    end_x = pos[0] + text_size[0][0] + 2\n",
    "    end_y = pos[1] + text_size[0][1] - 2\n",
    "    \n",
    "    cv2.rectangle(img,pos,(end_x,end_y),bg_color,cv2.FILLED)\n",
    "    cv2.putText(img,text,pos,cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,0),1,cv2.LINE_AA)\n",
    "    \n",
    "\n",
    "haar = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "def detect_face(img):\n",
    "    \n",
    "    coords = haar.detectMultiScale(img) # Detecting face using HaarCascade Classifier\n",
    "    \n",
    "    return coords\n",
    "\n",
    "cap =cv2.VideoCapture(0,apiPreference=cv2.CAP_AVFOUNDATION)\n",
    "#while cap.isOpened():\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # call the detection method\n",
    "    img = cv2.resize(frame,(224,224))\n",
    "    y_pred = detect_face_mask(img)\n",
    "    #print(y_pred)\n",
    "    \n",
    "    coords= detect_face(cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY))\n",
    "    \n",
    "    for x,y,w,h in coords:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "    if y_pred == 0:\n",
    "        draw_label(frame, \"Face mask\", (30,30),(0,255,0))\n",
    "    else:\n",
    "        draw_label(frame, \"No mask\", (30,30),(0,0,255))\n",
    "    \n",
    "    cv2.imshow(\"window\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('x'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c309f812",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The project demonstrates the use of computer vision and machine learning techniques to detect face masks in real-time. The project utilizes the VGG16 model for feature extraction and the HaarCascade classifier for face detection. The project can be further improved by using other pre-trained models or by training a custom model on a larger dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
